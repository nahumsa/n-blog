<!DOCTYPE html>
<html>

  <head>
  
    





  

  <meta charset="utf-8">  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="/n-blog/assets/images/favicon.ico" type="image/png" />    
  <title>Creating a simple Neural Network in JAX</title>
  
  <meta name="description" content="You can run this on colab: ">
  <link rel="stylesheet" href="/n-blog/css/main.css">
  <link rel="canonical" href="http://localhost:4000/n-blog/2020/09/25/NN-JAX.html">
  <link rel="alternate" type="application/rss+xml" title="N-Blog" href="http://localhost:4000/n-blog/feed.xml">
  <meta name="google-site-verification" content="fqeWQrskF38vWHtVyzcKHELwEGZiP9JesnM-sXbBu9Y" />
  <!-- For Latex -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-170504520-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    <a class="site-title" href="/n-blog/">N-Blog</a>

    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <nav class="site-nav">
        <a class="page-link" href="/n-blog/about/">&#x1f4ee; About</a>
      </nav>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Creating a simple Neural Network in JAX</h1>
    <p class="post-meta"><time datetime="2020-09-25T07:00:00-03:00" itemprop="datePublished">Sep 25, 2020</time></p>
    <span>[
      
        
        <a href="/n-blog/tag/Neural-Networks"><code class="highligher-rouge"><nobr>Neural-Networks</nobr></code>&nbsp;</a>
      
    ]</span>
    
  </header>
  
  <div class="post-content" itemprop="articleBody">
    <p>You can run this on colab: <a href="https://colab.research.google.com/github/nahumsa/JAX/blob/master/Simple%20NN%20JAX.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<h1 id="creating-a-neural-network-in-jax">Creating a neural network in JAX</h1>

<p><a href="https://github.com/google/jax">JAX</a> is a new python library that offers autograd and XLA, leading to high-performance machine learning, and numeric research. JAX works just as numpy and using jit (just in time) compilation, you can have high-performance without going to low level languages. One awesome thing is that, just as tensorflow, you can use GPUs and TPUs for acceleration.</p>

<p>In this post my aim is to build and train a simple Convolutional Neural Network using JAX.</p>

<h1 id="1-using-vmap-grad-and-jit">1) Using vmap, grad and jit</h1>

<h2 id="11-jit">1.1) jit</h2>

<p>In order to speed up your code, you can use the jit decorator, <code class="highlighter-rouge">@jit</code> which will cached your operation. Let’s compare the speed with and without jit. This example is taken from the <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html">Jax Quickstart Guide</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.67</span><span class="p">,</span> <span class="n">lmbda</span><span class="o">=</span><span class="mf">1.05</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">lmbda</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000000</span><span class="p">,))</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">selu</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jit</span>
<span class="n">selu_jit</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">selu</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">selu_jit</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The slowest run took 23.63 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 1.46 ms per loop
</code></pre></div></div>

<p>We see that with jit, we go 6 ms faster than without jit. Another remark is that we put the <code class="highlighter-rouge">block_until_ready()</code> method because asynchronous update by default.</p>

<h2 id="12-grad">1.2) grad</h2>

<p>Taking the gradient in JAX is pretty easy, you just need to call the <code class="highlighter-rouge">grad</code> function from the JAX library. Let’s begin with a simple example that is calculating the grad of $x^2$. From calculus, we know that:</p>

<script type="math/tex; mode=display">\frac{\partial x^2}{\partial x} = 2 x</script>

<script type="math/tex; mode=display">\frac{\partial^2 x^2}{\partial x^2} = 2</script>

<script type="math/tex; mode=display">\frac{\partial^3 x^2}{\partial x^3} = 0</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="n">square</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">grad_square</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">square</span><span class="p">)</span>
<span class="n">grad_grad_square</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">square</span><span class="p">))</span>
<span class="n">grad_grad_grad_square</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">square</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"grad 2² = "</span><span class="p">,</span> <span class="n">grad_square</span><span class="p">(</span><span class="mf">2.</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"grad grad 2² = "</span><span class="p">,</span> <span class="n">grad_grad_square</span><span class="p">(</span><span class="mf">2.</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"grad grad grad 2² = "</span><span class="p">,</span> <span class="n">grad_grad_grad_square</span><span class="p">(</span><span class="mf">2.</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>grad 2² =  4.0
grad grad 2² =  2.0
grad grad grad 2² =  0.0
</code></pre></div></div>

<h2 id="13-vmap">1.3) vmap</h2>

<p>vmap, or vectorizing map, maps a function along array axes, having better performance mainly when is composed with jit. Let’s apply this for matrix-vector products.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mat</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">batched_x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">apply_matrix</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<p>In order to batch naively, we can use a for loop to batch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">naively_batched_apply_matrix</span><span class="p">(</span><span class="n">v_batched</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">apply_matrix</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">v_batched</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Naively batched'</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">naively_batched_apply_matrix</span><span class="p">(</span><span class="n">batched_x</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Naively batched
100 loops, best of 3: 4.63 ms per loop
</code></pre></div></div>

<p>Now we can use vmap to batch our multiplication</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vmap</span>

<span class="o">@</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">vmap_batched_apply_matrix</span><span class="p">(</span><span class="n">v_batched</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">vmap</span><span class="p">(</span><span class="n">apply_matrix</span><span class="p">)(</span><span class="n">v_batched</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Auto-vectorized with vmap'</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">vmap_batched_apply_matrix</span><span class="p">(</span><span class="n">batched_x</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Auto-vectorized with vmap
The slowest run took 57.86 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 281 µs per loop
</code></pre></div></div>

<p>Now we can apply this for creating neural networks.</p>

<h1 id="2--using-stax-for-convolutional-neural-networks">2 ) Using STAX for Convolutional Neural Networks</h1>

<p>As a first example, we shall use MNIST (as always) to train a convolutional neural network using stax. It is important to import the original numpy package for shuffling and random generation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">onp</span>
</code></pre></div></div>

<p>Let’s import MNIST using <code class="highlighter-rouge">tensorflow_datasets</code> and transform the data into a np.array.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s">'/tmp/tfds'</span>
<span class="n">mnist_data</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"mnist"</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">mnist_data</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">mnist_data</span><span class="p">)</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s">'train'</span><span class="p">],</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s">'test'</span><span class="p">]</span>
<span class="n">num_labels</span> <span class="o">=</span> <span class="n">info</span><span class="p">.</span><span class="n">features</span><span class="p">[</span><span class="s">'label'</span><span class="p">].</span><span class="n">num_classes</span>
<span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">info</span><span class="p">.</span><span class="n">features</span><span class="p">[</span><span class="s">'image'</span><span class="p">].</span><span class="n">shape</span>
<span class="n">num_pixels</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span> <span class="n">c</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="n">clear_output</span><span class="p">()</span>
</code></pre></div></div>

<p>Let’s split the training and test dataset and one hot encode the labels of our data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="s">"""Create a one-hot encoding of x of size k """</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Full train set
</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s">'image'</span><span class="p">],</span> <span class="n">train_data</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">train_labels</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">)</span>

<span class="c1"># Full test set
</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s">'image'</span><span class="p">],</span> <span class="n">test_data</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we need to construct a data_stream which will generate our batch data, this data stream will shuffle the training dataset. First let’s define the batch size and how many batches should be used for going through all the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">num_train</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_complete_batches</span><span class="p">,</span> <span class="n">leftover</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">num_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_batches</span> <span class="o">=</span> <span class="n">num_complete_batches</span> <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">leftover</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">data_stream</span><span class="p">():</span>
  <span class="s">"""Creates a data stream with a predifined batch size.
  """</span>
  <span class="n">rng</span> <span class="o">=</span> <span class="n">onp</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">num_train</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
      <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]</span>
      <span class="k">yield</span> <span class="n">train_images</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">data_stream</span><span class="p">()</span>
</code></pre></div></div>

<p>Now let’s construct our network, we will contruct a simple convolutional neural network with 4 convoclutional blocks with batchnorm and relu and a dense softmax as output of the neural network.</p>

<p>First you define your neural network using <code class="highlighter-rouge">stax.serial</code> and get the init_fun and conv_net, the former is the initialization function of the network and the latter is your neural network which we will use on the update function.</p>

<p>After defining our network, we initialize it using the init function and we get our network parameters which we will optimize.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">stax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">jax.experimental.stax</span> <span class="kn">import</span> <span class="p">(</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">Conv</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span>
                                   <span class="n">Relu</span><span class="p">,</span> <span class="n">LogSoftmax</span><span class="p">)</span>

<span class="n">init_fun</span><span class="p">,</span> <span class="n">conv_net</span> <span class="o">=</span> <span class="n">stax</span><span class="p">.</span><span class="n">serial</span><span class="p">(</span><span class="n">Conv</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">"SAME"</span><span class="p">),</span>
                                 <span class="n">BatchNorm</span><span class="p">(),</span> <span class="n">Relu</span><span class="p">,</span>
                                 <span class="n">Conv</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">"SAME"</span><span class="p">),</span>
                                 <span class="n">BatchNorm</span><span class="p">(),</span> <span class="n">Relu</span><span class="p">,</span>
                                 <span class="n">Conv</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">"SAME"</span><span class="p">),</span>
                                 <span class="n">BatchNorm</span><span class="p">(),</span> <span class="n">Relu</span><span class="p">,</span>
                                 <span class="n">Conv</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">"SAME"</span><span class="p">),</span> <span class="n">Relu</span><span class="p">,</span>
                                 <span class="n">Flatten</span><span class="p">,</span>
                                 <span class="n">Dense</span><span class="p">(</span><span class="n">num_labels</span><span class="p">),</span>
                                 <span class="n">LogSoftmax</span><span class="p">)</span>


<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">init_fun</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="c1"># -1 for varying batch size
</span></code></pre></div></div>

<p>Now let’s define the accuracy and the loss function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
  <span class="s">""" Calculates the accuracy in a batch.

  Args:
    params : Neural network parameters.
    batch : Batch consisting of images and labels.
  
  Outputs:
    (float) : Mean value of the accuracy.
  """</span>

  <span class="c1"># Unpack the input and targets
</span>  <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span>
  
  <span class="c1"># Get the label of the one-hot encoded target
</span>  <span class="n">target_class</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  
  <span class="c1"># Predict the class of the batch of images using 
</span>  <span class="c1"># the conv_net defined before
</span>  <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">conv_net</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predicted_class</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
  <span class="s">""" Cross entropy loss.
  Args:
    params : Neural network parameters.
    batch : Batch consisting of images and labels.
  
  Outputs:
    (float) : Sum of the cross entropy loss over the batch.
  """</span>
  <span class="c1"># Unpack the input and targets
</span>  <span class="n">images</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span>
  <span class="c1"># precdict the class using the neural network
</span>  <span class="n">preds</span> <span class="o">=</span> <span class="n">conv_net</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>

  <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">*</span> <span class="n">targets</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s define which optimizer we shall use for training our neural network. Here we shall select the adam optimizer and initialize the optimizer with our neural network parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">optimizers</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">get_params</span> <span class="o">=</span> <span class="n">optimizers</span><span class="p">.</span><span class="n">adam</span><span class="p">(</span><span class="n">step_size</span><span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div>

<p>In order to create our update function for the network, we shall use the <code class="highlighter-rouge">jit</code> decorator to make things faster.</p>

<p>Inside the update function we take the value and gradient of the loss function given for the given parameters and the dataset and update our parameters using the optimizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span> <span class="n">value_and_grad</span>

<span class="o">@</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">):</span>
    <span class="s">""" Compute the gradient for a batch and update the parameters """</span>
    
    <span class="c1"># Take the gradient and evaluate the loss function
</span>    <span class="n">value</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    
    <span class="c1"># Update the network using the gradient taken
</span>    <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_update</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">),</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">value</span>
</code></pre></div></div>

<p>Now we shall create a training loop for the neural network, we run the loop for a number of epochs and run on all data using the data_stream that we defined before.</p>

<p>Then we record the loss and accuracy for each epoch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>


<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span> 
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>    
    <span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">_loss</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
    

  <span class="c1"># Update parameters of the Network
</span>  <span class="n">params</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>

  <span class="n">train_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_images</span><span class="p">))</span>
  <span class="n">val_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_images</span><span class="p">))</span>

  <span class="n">train_acc_epoch</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">))</span>
  <span class="n">test_acc_epoch</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>
  
  <span class="n">train_acc</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc_epoch</span><span class="p">)</span>
  <span class="n">test_acc</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc_epoch</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_gridspec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p style="text-align:center;"><img src="/n-blog/assets/2020-09-25-NN-JAX_files/2020-09-25-NN-JAX_35_0.png" /></p>

<p>Now we have successfully created and trained a neural network using JAX!</p>

<hr />

<h1 id="references">References</h1>

<ul>
  <li>
    <p><a href="https://roberttlange.github.io/posts/2020/03/blog-post-10/">Robert Lang Blog</a></p>
  </li>
  <li>
    <p><a href="https://jax.readthedocs.io/en/stable/notebooks/quickstart.html">JAX Quickstart</a></p>
  </li>
  <li>
    <p><a href="https://github.com/google/jax/blob/master/docs/notebooks/neural_network_with_tfds_data.ipynb">Training a Simple Neural Network, with tensorflow/datasets Data Loading</a></p>
  </li>
</ul>

  </div>

</article>

      </div>
    </div>    

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">N-Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>N-Blog</li>
          <li><a href="mailto:nahumsa@cbpf.br">nahumsa@cbpf.br</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/nahumsa"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">nahumsa</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/sa_nahum"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">sa_nahum</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Here I will share thoughts and ideas about Machine Learning  and Quantum Computing.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
